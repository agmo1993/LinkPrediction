{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Setting up data environment \n",
    "------\n",
    "The author attribute information (\"nodes.json\") was importated and loaded into a pandas Dataframe.\n",
    "\n",
    "Networkx graph library was used to store the training data, which was parsed from a txt file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "#Author attribitutes loaded\n",
    "\n",
    "nodefile = open(\"nodes.json\",\"r\")\n",
    "nodes = json.load(nodefile)\n",
    "\n",
    "Attribute_dataframe = pd.DataFrame()\n",
    "for i in nodes:\n",
    "    Attribute_dataframe = Attribute_dataframe.append(i,ignore_index=True)\n",
    "    \n",
    "\n",
    "#fill NaNs in attributes database\n",
    "Attribute_dataframe = Attribute_dataframe.fillna(0)\n",
    "Attribute_dataframe= Attribute_dataframe.set_index(\"id\")\n",
    "\n",
    "    \n",
    "    \n",
    "G = nx.Graph()\n",
    "for i in Attribute_dataframe.index:\n",
    "    node = i\n",
    "    G.add_node(int(node))\n",
    "\n",
    "network_graph = open(\"train.txt\",\"r\")\n",
    "network_graph = network_graph.readlines()\n",
    "\n",
    "for i in network_graph:\n",
    "    line = i.split(\" \")\n",
    "    node = line[0]\n",
    "    edges = line[1]\n",
    "    edges = edges.rstrip(\"\\n\")\n",
    "    edges = edges.split(\"\\t\")\n",
    "    for i in edges:\n",
    "        G.add_edge(int(node),int(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Step 2 - Creating negative edges set\n",
    "----\n",
    "Edges that are not present in the training graph are determined. The edges which have maximum hop length of 2 are removed from the edges set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "comb = combinations(G.nodes,2)\n",
    "\n",
    "missing_edges = []\n",
    "\n",
    "for i in comb:\n",
    "    if i not in G.edges:\n",
    "        missing_edges.append(i)\n",
    "\n",
    "\n",
    "missing_test_data = []\n",
    "for i in missing_edges:\n",
    "    try:\n",
    "        if nx.shortest_path_length(G,source=i[0],target=i[1]) > 2:\n",
    "            missing_test_data.append(i)\n",
    "    except:\n",
    "        print(str(i) + \"has no path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Step 3 - Feature data development of the training set\n",
    "----\n",
    "23000 edges are randomly selected from the missing edge set. Network features are computed for each of the missing edges and the training data set is placed into a pandas DataFrame. \n",
    "\n",
    "Network features of the edges in the training set (positives) are calculated and the training data set is placed into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "test_sample_size = 23000\n",
    "missing_test_data_selected = random.sample(missing_test_data,test_sample_size)\n",
    "\n",
    "\n",
    "network_data_table = pd.DataFrame()\n",
    "\n",
    "count = 0\n",
    "\n",
    "for i in missing_test_data_selected:\n",
    "    print(count)\n",
    "    source = i[0]\n",
    "    sink = i[1]\n",
    "    network_data_table = network_data_table.set_value(count,\"Source\",i[0])\n",
    "    network_data_table = network_data_table.set_value(count,\"Sink\",i[1])\n",
    "    array = [(source,sink)]\n",
    "    preds = nx.resource_allocation_index(G,array)\n",
    "    for u,v,p in preds:\n",
    "        ra_score = p\n",
    "    network_data_table = network_data_table.set_value(count,\"ra_score\",ra_score)\n",
    "    preds = nx.jaccard_coefficient(G,array)\n",
    "    for u,v,p in preds:\n",
    "        jc_score = p\n",
    "    network_data_table = network_data_table.set_value(count,\"jc_score\",jc_score)\n",
    "    preds = nx.preferential_attachment(G,array)\n",
    "    for u,v,p in preds:\n",
    "        pa_score = p\n",
    "    network_data_table = network_data_table.set_value(count,\"pa_score\",pa_score)\n",
    "    preds = nx.adamic_adar_index(G,array)\n",
    "    for u,v,p in preds:\n",
    "        pa_score = p\n",
    "    network_data_table = network_data_table.set_value(count,\"aa_score\",pa_score)\n",
    "    cn_num = len(sorted(nx.common_neighbors(G, source, sink)))\n",
    "    network_data_table = network_data_table.set_value(count,\"cn_num\",cn_num)\n",
    "    network_data_table = network_data_table.set_value(count,\"Class\",0)\n",
    "    count += 1\n",
    "\n",
    "for i in G.edges():\n",
    "    if i in G.edges():\n",
    "        print(count)\n",
    "        network_data_table = network_data_table.set_value(count,\"Source\",i[0])\n",
    "        network_data_table = network_data_table.set_value(count,\"Sink\",i[1])\n",
    "        source = i[0]\n",
    "        sink = i[1]\n",
    "        array = [(source,sink)]\n",
    "        preds = nx.resource_allocation_index(G,array)\n",
    "        for u,v,p in preds:\n",
    "            ra_score = p\n",
    "        network_data_table = network_data_table.set_value(count,\"ra_score\",ra_score)\n",
    "        preds = nx.jaccard_coefficient(G,array)\n",
    "        for u,v,p in preds:\n",
    "            jc_score = p\n",
    "        network_data_table = network_data_table.set_value(count,\"jc_score\",jc_score)\n",
    "        preds = nx.preferential_attachment(G,array)\n",
    "        for u,v,p in preds:\n",
    "            pa_score = p\n",
    "        network_data_table = network_data_table.set_value(count,\"pa_score\",pa_score)\n",
    "        preds = nx.adamic_adar_index(G,array)\n",
    "        for u,v,p in preds:\n",
    "            pa_score = p\n",
    "        network_data_table = network_data_table.set_value(count,\"aa_score\",pa_score)\n",
    "        cn_num = len(sorted(nx.common_neighbors(G, source, sink)))\n",
    "        network_data_table = network_data_table.set_value(count,\"cn_num\",cn_num)\n",
    "        network_data_table = network_data_table.set_value(count,\"AClass\",1)\n",
    "        count += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Step 4 - Feature data development of the training set\n",
    "----\n",
    "The data is transformed transformed from a pandas dataframe to a numpy array, shuffled and then in to seperate arrays for x and y values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_values_table = network_values_table.drop([\"Source\",\"Sink\"],axis=1)\n",
    "network_values_table = network_values_table[sorted(network_values_table.columns)]\n",
    "values = network_values_table.values\n",
    "np.random.shuffle(values)\n",
    "\n",
    "y = np.empty([training_sample_size+test_sample_size,1])\n",
    "count = 0\n",
    "x_value = np.empty([training_sample_size+test_sample_size,5])\n",
    "for i in values:\n",
    "    y[count] = i[0]\n",
    "    x_value[count] = i[1:0]\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Step 5 - Feature data development of the training set\n",
    "----\n",
    "The x and y are fit to a Logistic regression model from sklearn, using three fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegressionCV(cv=3)\n",
    "logreg.fit(x_value, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Step 6 - Feature data development of the testing set \n",
    "----\n",
    "The network features are calculated for the edges in the test set, taken from \"test-public.csv\".\n",
    "\n",
    "Th pandas dataframe is transformed to a numpy array, which can be used by the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_original = pd.read_csv(\"test-public.csv\")\n",
    "test_file_original.set_index(\"Id\")\n",
    "\n",
    "sample_set = pd.DataFrame()\n",
    "\n",
    "for i in test_file_original.index:\n",
    "    source = test_file_original[\"Source\"][i]\n",
    "    sink = test_file_original[\"Sink\"][i]\n",
    "    #sample_set = sample_set.set_value(i,\"Source\",source)\n",
    "    #sample_set = sample_set.set_value(i,\"Sink\",sink)\n",
    "    array = [(source,sink)]\n",
    "    preds = nx.resource_allocation_index(G,array)\n",
    "    for u,v,p in preds:\n",
    "        ra_score = p\n",
    "    sample_set = sample_set.set_value(i,\"ra_score\",ra_score)\n",
    "    preds = nx.jaccard_coefficient(G,array)\n",
    "    for u,v,p in preds:\n",
    "        jc_score = p\n",
    "    sample_set = sample_set.set_value(i,\"jc_score\",jc_score)\n",
    "    preds = nx.preferential_attachment(G,array)\n",
    "    for u,v,p in preds:\n",
    "        pa_score = p\n",
    "    sample_set = sample_set.set_value(i,\"pa_score\",pa_score)\n",
    "    cn_num = len(sorted(nx.common_neighbors(G, source, sink)))\n",
    "    sample_set = sample_set.set_value(i,\"cn_num\",cn_num)  \n",
    "    preds = nx.adamic_adar_index(G,array)\n",
    "    for u,v,p in preds:\n",
    "        pa_score = p\n",
    "    sample_set = sample_set.set_value(count,\"aa_score\",aa_score)\n",
    "\n",
    "sample_set = sample_set[sorted(sample_set.columns)]\n",
    "sample_value = sample_set.values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Step 7 - Prediction of test data \n",
    "----\n",
    "The Logistic regression model is used to predict the probability of an edge (Class 1) on the test edges. The results are loaded into the test file for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_original = pd.read_csv(\"test-public.csv\")\n",
    "test_file_original.set_index(\"Id\")\n",
    "\n",
    "for i in sample_set.index:\n",
    "    try:\n",
    "        alpha = 0\n",
    "        array = sample_values[i]\n",
    "        score_nnet = logreg.predict_proba(np.array(array,ndmin=2))\n",
    "        score_nnet = score_nnet[0][1]\n",
    "        print(str(i) + \"LR Score is \" + str(score_nnet))\n",
    "        test_file_original.set_value(i,\"Predicted\",score_nnet)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "test_file_original = test_file_original[[\"Id\",\"Predicted\"]]\n",
    "test_file_original.to_csv(\"test_file_results_LR.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
